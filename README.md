# Data Engineer Nanodegree - Project 3: Data Warehouse on AWS

- [Data Engineer Nanodegree - Project 3: Data Warehouse on AWS](#data-engineer-nanodegree---project-3-data-warehouse-on-aws)
  - [Summary](#summary)
  - [Dataset](#dataset)
  - [Schema](#schema)
  - [ETL Pipeline](#etl-pipeline)
  - [How to run](#how-to-run)
    - [1. Config file](#1-config-file)
  - [2. Choose input source](#2-choose-input-source)
  - [Purpose of the database](#purpose-of-the-database)

## Summary

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Dataset

The project is based on two dataset:
- **Song dataset**: it is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

     song_data/A/B/C/TRABCEI128F424C983.json
     song_data/A/A/B/TRAABJL12903CDCF1A.json



    And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

    ```json
    {
        "num_songs": 1,
        "artist_id": "ARJIE2Y1187B994AB7",
        "artist_latitude": null,
        "artist_longitude": null,
        "artist_location": "",
        "artist_name": "Line Renaud",
        "song_id": "SOUPIRU12A6D4FA1E1",
        "title": "Der Kleine Dompfaff",
        "duration": 152.92036,
        "year": 0
    }
    ```

- **Log Dataset**: it consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

    The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

    ```
    log_data/2018/11/2018-11-12-events.json
    log_data/2018/11/2018-11-13-events.json
    ```

## Schema
In order to analyze and query the data, Sparkify wants to create an OLAP database with PostgreSQL. After the etl process, the star schema resulting in the database will be the following:

**Staging tables**
The staging table have the same schema of the song files and the log files.

**Songoplays table** (Fact table)
| Column name | Type      | Properties        |
| ----------- | --------- | ----------------- |
| songplay_id | BIGINT    | PK, AUTOINCREMENT |
| start_time  | timestamp |                   |
| user_id     | int       |                   |
| level       | varchar   |                   |
| song_id     | varchar   |                   |
| artist_id   | varchar   |                   |
| session_id  | int       |                   |
| location    | varchar   |                   |
| user_agent  | varchar   |                   |

**Songs**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| song_id     | varchar | PK         |
| title       | varchar |            |
| artist_id   | varchar |            |
| year        | int     |            |
| duration    | numeric |            |

**Artists**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| artist_id   | varchar | PK         |
| name        | varchar |            |
| location    | varchar |            |
| latitude    | numeric |            |
| longitude   | numeric |            |

**Users**
| Column name | Type    | Properties |
| ----------- | ------- | ---------- |
| user_id     | varchar | PK         |
| firsname    | varchar |            |
| lastname    | varchar |            |
| gender      | char    |            |
| level       | varchar |            |

**Time**
| Column name | Type      | Properties |
| ----------- | --------- | ---------- |
| start_time  | timestamp | PK         |
| hour        | int       |            |
| day         | int       |            |
| week        | int       |            |
| month       | int       |            |
| year        | int       |            |
| weekday     | int       |            |

## ETL Pipeline

The ETL pipeline, in this case, is a Spark job that reads all the json files (songs and logs) and create the star schema described before. The tables are exported as Parquet files.

## How to run

### 1. Config file
Fill the `dl.cfg` file with the correct informations.

### 2. Choose input source
In the etl.py script, make sure to select the right input for you. In the main function there are two pairs of dicts: one is for local data and another is for S3 data.

## Purpose of the database

Sparkify wants to create this database to analyze and get insight from the data collected by their app. In fact, the app collects a lot of user actions, but the proble is: how to process all these data and how to get infromations from this logs? Creating an OLAP db like this, Sparkify can query the database, aggregate data and understand what songs users are listening.

Looking at the database, some possible and useful queries that Sparkify's team can execute are:
- Trending songs in a certain location
    ```SQL
    SELECT s.song_id, title, count(*) as plays
    FROM songplays AS sp JOIN songs AS s ON sp.song_id = s.song_id
    WHERE location = 'Chicago-Naperville-Elgin, IL-IN-WI'
    GROUP BY s.song_id
    ORDER BY plays DESC
    ```
- Trending artists in a certain location (similar to the previous query)
- How many songs are listened per session by user with free and paid level
    ```sql
    SELECT level, avg(songs)
    FROM (
            SELECT session_id, level, count(*) AS songs
            FROM songplays
            GROUP BY session_id, level
        ) AS t1
    GROUP BY level
    ```
- Top 20 songs for specific a year
  ```sql
  SELECT sp.song_id, title, count(*) AS plays
  FROM songplays AS sp JOIN songs AS s ON sp.song_id = s.song_id
    JOIN time t ON sp.start_time = t.start_time
  WHERE t.year = 2018
  GROUP BY sp.song_id, title
  ORDER BY plays DESC
  LIMIT 20

  ```

A lot of other queries can be executed to analyze the populatiry of songs and artists, also using geographic information, or analyze user behavior at a specific period of the day/month/week.
